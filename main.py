import os
import json
import base64
import warnings
import time
from collections import defaultdict
from pathlib import Path
from dotenv import load_dotenv
from google.genai.types import (
    Part,
    Content,
    Blob,
)
from sarvamai import AsyncSarvamAI, AudioOutput 
from google.adk.runners import InMemoryRunner
from google.adk.agents import LiveRequestQueue
from google.adk.agents.run_config import RunConfig
from google.genai import types

from fastapi import FastAPI, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

from google_search_agent.agent import root_agent
#from elevenlabs.client import ElevenLabs
from sarvamai import AsyncSarvamAI, AudioOutput 

warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

# Load Gemini API Key
load_dotenv()

APP_NAME = "ADK Streaming example"

timings = defaultdict(dict)


SARVAM_API_KEY = os.getenv("SARVAM_API_KEY", "")
SARVAM_SPEAKER = os.getenv("SARVAM_SPEAKER", "anushka")     # voice
SARVAM_MODEL_ID = os.getenv("SARVAM_MODEL_ID", "bulbul:v2") # model
SARVAM_LANGUAGE_CODE = os.getenv("SARVAM_LANGUAGE_CODE", "en-IN")
SARVAM_OUTPUT_CODEC = os.getenv("SARVAM_OUTPUT_CODEC", "linear16")  # keeps audio/pcm


sarvam_client = None
if SARVAM_API_KEY:
    sarvam_client = AsyncSarvamAI(api_subscription_key=SARVAM_API_KEY)


async def start_agent_session(user_id, is_audio=False):
    """Starts an agent session"""

    # Create a Runner
    runner = InMemoryRunner(
        app_name=APP_NAME,
        agent=root_agent,
    )

    # Create a Session
    session = await runner.session_service.create_session(
        app_name=APP_NAME,
        user_id=user_id,  # Replace with actual user ID
    )

    # Always request TEXT from Gemini; we'll synthesize audio via ElevenLabs when needed
    modality = "TEXT"
    run_config = RunConfig(
        response_modalities=[modality],
        session_resumption=types.SessionResumptionConfig()
    )

    # Create a LiveRequestQueue for this session
    live_request_queue = LiveRequestQueue()

    # Start agent session
    live_events = runner.run_live(
        session=session,
        live_request_queue=live_request_queue,
        run_config=run_config,
    )
    return live_events, live_request_queue


async def stream_sarvam_tts(text: str):
    """Stream PCM audio generated by Sarvam.ai as SSE audio/pcm frames (LINEAR16)."""
    if not sarvam_client or not SARVAM_SPEAKER:
        return

    try:
        async with sarvam_client.text_to_speech_streaming.connect(
            model=SARVAM_MODEL_ID,
            send_completion_event=True,
        ) as ws:
            # Configure voice + format (linear16 keeps 'audio/pcm' on the wire)
            await ws.configure(
                target_language_code=SARVAM_LANGUAGE_CODE,
                speaker=SARVAM_SPEAKER,
                output_audio_codec=SARVAM_OUTPUT_CODEC,  # "linear16"
                
            )

            # Send full turn text and flush
            await ws.convert(text)
            await ws.flush()

            # Stream audio chunks
            async for msg in ws:
                # Audio chunk
                if isinstance(msg, AudioOutput):
                    try:
                        chunk = base64.b64decode(msg.data.audio)
                    except Exception:
                        continue
                    message = {
                        "mime_type": "audio/pcm",
                        "data": base64.b64encode(chunk).decode("ascii"),
                    }
                    yield f"data: {json.dumps(message)}\n\n"
                    print(f"[AGENT TO CLIENT via Sarvam]: audio/pcm: {len(chunk)} bytes.")
                    continue

                # Completion event -> end this TTS stream promptly
                if isinstance(msg, EventResponse):
                    try:
                        if getattr(msg.data, "event_type", None) == "final":
                            break
                    except Exception:
                        pass
                    continue

                # Error from service -> log and stop this turn's stream
                if isinstance(msg, ErrorResponse):
                    print(f"Error from Sarvam TTS stream: {getattr(msg, 'message', msg)}")
                    break
    except Exception as e:
        print(f"Error streaming Sarvam TTS: {e}")


async def agent_to_client_sse(live_events, synthesize_audio: bool,user_id_str: str):
    """Agent to client communication via SSE"""
    # Accumulate the current turn's text to synthesize on turn completion
    current_turn_text_parts = []
    timings.setdefault(user_id_str, {})
    async for event in live_events:
        # If the turn complete or interrupted, send it
        if event.turn_complete or event.interrupted:
            timings[user_id_str]["turn_complete"] = time.time()
            # Seed fallbacks so first turn always prints (even with no partials)
            t = timings[user_id_str]
            t.setdefault("speech_start", t["turn_complete"])
            t.setdefault("first_partial", t["turn_complete"])
            message = {
                "turn_complete": event.turn_complete,
                "interrupted": event.interrupted,
            }
            yield f"data: {json.dumps(message)}\n\n"
            print(f"[AGENT TO CLIENT]: {message}")

            # When a turn completes successfully, synthesize audio via ElevenLabs if requested
            if synthesize_audio and event.turn_complete and current_turn_text_parts:
                # Combine collected partials into final text
                final_text = "".join(current_turn_text_parts)
                # Stream ElevenLabs audio to the client over SSE
                
                async for audio_sse in stream_sarvam_tts(final_text):
                    yield audio_sse

                timings[user_id_str]["audio_done"] = time.time()

                t = timings[user_id_str]
                if "speech_start" in t:
                    e2e = t["audio_done"] - t["speech_start"]
                    ttfp = (t.get("first_partial", t["turn_complete"]) - t["speech_start"])
                    print(f"[METRICS] E2E={e2e:.3f}s | First-partial={ttfp:.3f}s")
                timings[user_id_str].clear()
            # Reset for next turn
            elif synthesize_audio and event.turn_complete and not current_turn_text_parts:
                final_text = ""
                if event.content and getattr(event.content, "parts", None):
                    final_text = "".join([p.text for p in event.content.parts if getattr(p, "text", None)])
                if final_text:
                    async for audio_sse in stream_sarvam_tts(final_text):
                        yield audio_sse
                    timings[user_id_str]["audio_done"] = time.time()
                    t = timings[user_id_str]
                    if "speech_start" in t:
                        e2e = t["audio_done"] - t["speech_start"]
                        print(f"[METRICS] E2E={e2e:.3f}s")
                    timings[user_id_str].clear()
            current_turn_text_parts = []
            continue

        # Read the Content and its first Part
        part: Part = (
            event.content and event.content.parts and event.content.parts[0]
        )
        if not part:
            continue

        # Ignore any audio chunks from Gemini; we synthesize with ElevenLabs

        # If it's text and a parial text, send it
        if part.text and event.partial:
            if "first_partial" not in timings[user_id_str]:
                timings[user_id_str]["first_partial"] = time.time()
            message = {
                "mime_type": "text/plain",
                "data": part.text
            }
            yield f"data: {json.dumps(message)}\n\n"
            print(f"[AGENT TO CLIENT]: text/plain: {message}")
            # Buffer the text for TTS at end of turn
            current_turn_text_parts.append(part.text)


#
# FastAPI web app
#

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

STATIC_DIR = Path("static")
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# Store active sessions
active_sessions = {}


@app.get("/")
async def root():
    """Serves the index.html"""
    return FileResponse(os.path.join(STATIC_DIR, "index.html"))


@app.get("/events/{user_id}")
async def sse_endpoint(user_id: int, is_audio: str = "false"):
    """SSE endpoint for agent to client communication"""

    # Start agent session
    user_id_str = str(user_id)
    live_events, live_request_queue = await start_agent_session(user_id_str, is_audio == "true")

    # Store the request queue for this user
    active_sessions[user_id_str] = live_request_queue

    print(f"Client #{user_id} connected via SSE, audio mode: {is_audio}")

    def cleanup():
        live_request_queue.close()
        if user_id_str in active_sessions:
            del active_sessions[user_id_str]
        print(f"Client #{user_id} disconnected from SSE")

    async def event_generator():
        try:
            # When client is in audio mode, synthesize via ElevenLabs
            async for data in agent_to_client_sse(live_events, synthesize_audio=(is_audio == "true"),user_id_str=user_id_str):
                yield data
        except Exception as e:
            print(f"Error in SSE stream: {e}")
        finally:
            cleanup()

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )



@app.post("/send/{user_id}")
async def send_message_endpoint(user_id: int, request: Request):
    """HTTP endpoint for client to agent communication"""

    user_id_str = str(user_id)

    # Get the live request queue for this user
    live_request_queue = active_sessions.get(user_id_str)
    if not live_request_queue:
        return {"error": "Session not found"}

    # Parse the message
    message = await request.json()
    mime_type = message["mime_type"]
    data = message["data"]

    # Send the message to the agent
    if mime_type == "text/plain":
        content = Content(role="user", parts=[Part.from_text(text=data)])
        live_request_queue.send_content(content=content)
        print(f"[CLIENT TO AGENT]: {data}")
    elif mime_type == "audio/pcm":
        decoded_data = base64.b64decode(data)
        live_request_queue.send_realtime(Blob(data=decoded_data, mime_type=mime_type))
        print(f"[CLIENT TO AGENT]: audio/pcm: {len(decoded_data)} bytes")
    else:
        return {"error": f"Mime type not supported: {mime_type}"}

    return {"status": "sent"}
